Notes from Assignment:
    Next state transitions are stochastic
        ->

    Goal:
        - uses Q-Learning to enable the robot to learn how to navigate from its starting
          location in the bottom left corner to the goal location in the top right corner.

    Alpha Value -> a decimal number between 0 and 1 to use for the learning rate ð›¼ in the Q-Learning update rule
        Note from Assignment:
            -> Note: if the user passes in a value of 0 for <alphaValue>, then your agent should instead use ð›¼ = 1 / ð‘›(ð‘ , ð‘Ž) where ð‘›(ð‘ , ð‘Ž) tracks the number of times the agent has thus far chosen action ð‘Ž in state ð‘ . For this assignment, fix ð›¾ = 0.99.
    Epsilon Value -> a decimal number between 0 and 1 to use for the exploration rate ðœ– in the ðœ–-greedy exploration-exploitation algorithm

    Grid Class:
        1) generateStartState(), which returns the starting state for the robot
        2) generateNextState(state, action), which takes in the current state and a
                chosen action as parameters and returns a random next state for the agent
        3) generateReward(state, action), which also takes in the current state and a chosen action as parameters and returns a reward for the agent (to use for reinforcing that action choice)


Important Notes:
    - Once the current state is equal to the ABSORBING_STATE, the agent has finished an episode
        - This happens after the agent either reaches the goal or the pit
        - MAKE SURE TO KEEP TRACK OF CUMULATIVE REWARD EARNED BY AGENT IN EPISODE
    - Use Epsilon greedy to choose our actions
    - Run agent 100 times each time program is executed and DO NOT reset the Q-Table [nor the ð‘›(ð‘ , ð‘Ž) values] between these 100 episodes

Implementation steps and needed information:
    -> Q-Learning:
        -> Q-Learning update rule
            - Generate
        -> ðœ–-greedy algorithm
            - chooses the action with the highest Q(s, a) with probability 1-Îµ

Stand Q-Learning Algorithm:
    Initialize the Q(s, a) values for known state/action pairs
        Often start at 0 or random values close to 0

    1. Observe the current state s
    2. Choose an action a based on the Q(s, a) values
    3. Observe the next state sâ€™ and reward r
    4. Update the value of Q(s, a) using s, a, sâ€™, r


    (s, a) => [(0,0), "up"]



Next steps:

making function for exploration to run the 100 iteration

change method for q learning to take in a q table so don't lose the values or create a new table that deletes them. # Allows us to learn from past itereations

Change action list to just append the new Q values for the action chosen from for the currentState instead of assigning right off the bat the [0.0, etc.]