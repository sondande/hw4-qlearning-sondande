Notes from Assignment:
    Next state transitions are stochastic
        ->

    Goal:
        - uses Q-Learning to enable the robot to learn how to navigate from its starting
          location in the bottom left corner to the goal location in the top right corner.

    Alpha Value -> a decimal number between 0 and 1 to use for the learning rate ğ›¼ in the Q-Learning update rule
        Note from Assignment:
            -> Note: if the user passes in a value of 0 for <alphaValue>, then your agent should instead use ğ›¼ = 1 / ğ‘›(ğ‘ , ğ‘) where ğ‘›(ğ‘ , ğ‘) tracks the number of times the agent has thus far chosen action ğ‘ in state ğ‘ . For this assignment, fix ğ›¾ = 0.99.
    Epsilon Value -> a decimal number between 0 and 1 to use for the exploration rate ğœ– in the ğœ–-greedy exploration-exploitation algorithm

    Grid Class:
        1) generateStartState(), which returns the starting state for the robot
        2) generateNextState(state, action), which takes in the current state and a
                chosen action as parameters and returns a random next state for the agent
        3) generateReward(state, action), which also takes in the current state and a chosen action as parameters and returns a reward for the agent (to use for reinforcing that action choice)


Important Notes:
    - Once the current state is equal to the ABSORBING_STATE, the agent has finished an episode
        - This happens after the agent either reaches the goal or the pit
        - MAKE SURE TO KEEP TRACK OF CUMULATIVE REWARD EARNED BY AGENT IN EPISODE
    - Use Epsilon greedy to choose our actions
    - Run agent 100 times each time program is executed and DO NOT reset the Q-Table [nor the ğ‘›(ğ‘ , ğ‘) values] between these 100 episodes

Implementation steps and needed information:
    -> Q-Learning:
        -> Q-Learning update rule
            - Generate
        -> ğœ–-greedy algorithm
            - chooses the action with the highest Q(s, a) with probability 1-Îµ

Stand Q-Learning Algorithm:
    Initialize the Q(s, a) values for known state/action pairs
        Often start at 0 or random values close to 0

    1. Observe the current state s
    2. Choose an action a based on the Q(s, a) values
    3. Observe the next state sâ€™ and reward r
    4. Update the value of Q(s, a) using s, a, sâ€™, r