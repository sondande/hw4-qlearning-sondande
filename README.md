[![Open in Visual Studio Code](https://classroom.github.com/assets/open-in-vscode-f059dc9a6f8d3a56e377f745f24479a46679e63a5d9fe6f495e02850cd0d8118.svg)](https://classroom.github.com/online_ide?assignment_repo_id=6597221&assignment_repo_type=AssignmentRepo)
# hw4-qlearning
HW4: Stochastic Robot Navigation (Q-Learning)

1. A paragraph (5-10 sentences) comparing the results in your line chart from the first experiment. What trends did you observe about the agentâ€™s learning? How did the agentâ€™s performance change as you varied the learning rate ğ›¼? Which learning rate did you find led to the best performance? Please make sure to list the five learning rates compared in your experiment.
2. A paragraph (5-10 sentences) comparing the results in your line chart from the second experiment. What trends did you observe about the agentâ€™s learning? How did the agentâ€™s performance change as you varied the exploration rate ğœ–? Which exploration rate did you find led to the best performance? Please make sure to list the five exploration rates compared in your experiment.
3. Based on your results from both experiments, what advice would you offer someone new to reinforcement learning about how to choose appropriate values for ğ›¼ and ğœ–?
4. A short paragraph describing your experience during the assignment (what did you enjoy, what was difficult, etc.)
   1. We Found this lab to be difficult but very interesting overall. Through the process of understanding Deep learning and the application, we could see how our program utiilized the information to learn about better paths 
      and costs when going down a path. At first, we were using an action list that held the first default values of a currentState that was not in our dictionary at first. We found that becasue we originally set up our 
      dictionary to take on multiple lists, it resulted in information being edited and changed. As a result, our program was able to learn from the data it was being given. We found a better method to find the best course of 
      action for our program to take alongside maintaining the values in out Q_table and being able to edit them accordingly based on our current state and generating new Q values through our Q-learning update rule section. 
   2. 
5. 25 hours 
6. We affirm that we have adhered to the honor code on this assignment. - Sagana Ondande & Oliver Rippen
