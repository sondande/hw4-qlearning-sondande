[![Open in Visual Studio Code](https://classroom.github.com/assets/open-in-vscode-f059dc9a6f8d3a56e377f745f24479a46679e63a5d9fe6f495e02850cd0d8118.svg)](https://classroom.github.com/online_ide?assignment_repo_id=6597221&assignment_repo_type=AssignmentRepo)
# hw4-qlearning
HW4: Stochastic Robot Navigation (Q-Learning)

1. A paragraph (5-10 sentences) comparing the results in your line chart from the first experiment. What trends did you observe about the agent‚Äôs learning? How did the agent‚Äôs performance change as you varied the learning rate ùõº? Which learning rate did you find led to the best performance? Please make sure to list the five learning rates compared in your experiment.
   1. From the results of our first experiment, we can see that there was an overall consistency in range throughout all learning rates and that the graph was learning overall. We thought that it was interest that the graph was spiking in the graph but came to the conclusion that
      this was showing how we the graph learned through each episode through the various passages but began to stay relatively consistent through their choices later in the iterations. We found that the best learning rate from our data came from when our alpha value was equal to 0.65 which was a surprise due to the results of when alpha was 035. 
      When looking at the graph, the values in the middle of the graph showed to have higher utilities found when the learning rate was 0.5 but plummeted near the end of our episode iterations. As a result, the 0.65 was what we found to be the most consistent. I believe the pattern between when we choose different rates was if the value was big enough so that the information 
      was useful. This reminds me of the saying "too much of a good thing is bad" and seeing that apply to the data of having a large learning rate requires more time to find the best values but has more opportunity to find worse utilities due to the overall accessibility of how far they are learning. This is interesting as we kept the controlled variable being the exploration aspect of 
      the equation. I think the closer the value was to 1, the more inconsistent the values became in terms of finding similar cumulative utilizes compared to when it was closer to 0. When the value was 0, the values were more consistent, but I believe that to be from the case of using the amount of times a current state chose an action as the denominator of the fraction leaving the learning rate 
      vary dependent on the choices made giving it more consistency.
2. A paragraph (5-10 sentences) comparing the results in your line chart from the second experiment. What trends did you observe about the agent‚Äôs learning? How did the agent‚Äôs performance change as you varied the exploration rate ùúñ? Which exploration rate did you find led to the best performance? Please make sure to list the five exploration rates compared in your experiment.
   1. From looking at the results from the second experiment, we could see a more consistent pattern in terms of the progression of the graph. It seems that the overall learning for the agent was more clear alongside converged more towards the end with having more consistent utility values later in the episode iteration. We found that through the varying exploration rates resulted in more variations in the beginning of our search 
      for when the value was larger compared to the smallest value we used of 0.01. This makes more sense in terms of limiting the search to ensure that we are only using areas we want and know are consistent to explore. As a result, our utility values did not have as many variations and not as many deep falls in the value compared to when our exploration rate was larger. As a result, the best learning rate we found from our data 
      was when epsilon was equal to 0.01 while our alpha value was 0.65.
3. Based on your results from both experiments, what advice would you offer someone new to reinforcement learning about how to choose appropriate values for ùõº and ùúñ?
   1. The advice we would offer to someone new to reinforcement learning is that finding a good alpha value is great overall as it will give you a good range of extents for utilities but the variation and overall consistency of data is determined by a small exploration value as that limits how much you want to dive away from better values. I would use a relatively even alpha value so that you gain a large enough range to find the best utility
      and choose a relatively small exploration value so that the choices made are more concise choices resulting in finding the best values faster, easier, and allows our iterations to meet convergence in a more efficient manner. Overall, a higher epsilon value results in a higher regret, due to there not being as much exploitation. With a lower epsilon value, it results in less regret, but will find the best performing variant faster and with 
      more accuracy.
4. A short paragraph describing your experience during the assignment (what did you enjoy, what was difficult, etc.)
   1. We Found this lab to be difficult but very interesting overall. Through the process of understanding Deep learning and the application, we could see how our program utilized the information to learn about better paths 
      and costs when going down a path. At first, we were using an action list that held the first default values of a currentState that was not in our dictionary at first. We found that because we originally set up our 
      dictionary to take on multiple lists, it resulted in information being edited and changed. As a result, our program was able to learn from the data it was being given. We found a better method to find the best course of 
      action for our program to take alongside maintaining the values in out Q_table and being able to edit them accordingly based on our current state and generating new Q values through our Q-learning update rule section.
5. 25 hours 
6. We affirm that we have adhered to the honor code on this assignment. - Sagana Ondande & Oliver Rippen
